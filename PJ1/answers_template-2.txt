Team: Project Group 5
1- Yang Xie
2- Haoying Dai
3- Da He

Email:

---------------------------------------------------------------------------------
Part 1)

1- baseline accuracy = 55.54%
2- Cohenâ€™s Kappa = 1 (completely agreed with gold annotations)

---------------------------------------------------------------------------------
Part 2)
1-

s   | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
c(s)|  273 |    253     |    251    |  312  |  1526   |  287


s        | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
c(s,time)|   12  |    13     |    16     |  15   |    43   |  19

s        | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
c(s,loss)|   1  |    0       |    0      |  2    |    23    |  0


s          | cord | division   | formation | phone | product | text
---------------------------------------------------------------------
c(s,export)|   0  |    0       |    0      |  1    |    3    |  0



2- (the following values in percentage %)

s   | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
p(s)|  9.4 |    8.7     |   8.6     |  10.7 |   52.6  |  9.9


s        | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
p(s|time)| 8.90 |  8.47      |   10.36   | 11.25 | 48.33   | 12.68

s        | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
p(s|loss)| 4.32 |  1.91      |   1.92    | 6.66  | 83.19   | 2.00


s          | cord | division   | formation | phone | product | text
---------------------------------------------------------------------
p(s|export)| 8.21 |   7.26     |   7.31    | 16.87 | 52.73  | 7.61


3- for the sentence X = "and i can tell you that i 'm an absolute nervous wreck every time she performs . i have her practice the last two lines on each page , so I can learn exactly when to turn the page -- just one of the tricks to this trade that i 've learned the hard way ."

s     | cord | division   | formation | phone | product | text
-----------------------------------------------------------------
p(s|X)| 0.03 |  6.1e-06   |  1.3e-04  |2.3e-03| 2.3e-09 | 99.97

4- classifier f-measures on the test set:
micro averaged = 0.8523
macro averaged = 0.7677

5-
smoothing: we used add 1 smoothing as suggested in the lecture.
log-probabilities: use log to improve numerical stability.
---------------------------------------------------------------------------------
Part 3)

1-
Wrong label:  cord
Weight change for wrong label:
[('.', -1), ('``', -1), ('her', -2), ('with', -1), ('plucky', -1), ('an', -1), ('to', -2), ('plank', -1), ('jean-jacques', -1), ('last', -1), ('by', -1), ('line', -1), ('tied', -1), ('painting', -1), ('sits', -1), ('drawing', -1), ('another', -1), ('the', -2), ('much-quoted', -1), ('not', -1), ('shows', -1), ('friend', -1), ('lady', -1), ('a', -1), ('madame', -1), ('room', -1), ('before', -1), (',', -3), ('dog', -1), ('in', -1), ('down', -1), ('managed', -1), ('did', -1), ('french', -1), ('little', -1), ('rolland', -1), ('!', -1), ('exquisite', -1), ('harp', -1), ('pet', -1), ('ah', -1), ('liberty', -1), ('she', -1), ('and', -1), ('of', -1), ('hauer', -1), ('lafayette', -1), ('who', -1), ('wordsworth', -1)]
Correct label:  text
Weight change for correct label:
[('.', 1), ('``', 1), ('her', 2), ('with', 1), ('plucky', 1), ('an', 1), ('to', 2), ('plank', 1), ('jean-jacques', 1), ('last', 1), ('by', 1), ('line', 1), ('tied', 1), ('painting', 1), ('sits', 1), ('drawing', 1), ('another', 1), ('the', 2), ('much-quoted', 1), ('not', 1), ('shows', 1), ('friend', 1), ('lady', 1), ('a', 1), ('madame', 1), ('room', 1), ('before', 1), (',', 3), ('dog', 1), ('in', 1), ('down', 1), ('managed', 1), ('did', 1), ('french', 1), ('little', 1), ('rolland', 1), ('!', 1), ('exquisite', 1), ('harp', 1), ('pet', 1), ('ah', 1), ('liberty', 1), ('she', 1), ('and', 1), ('of', 1), ('hauer', 1), ('lafayette', 1), ('who', 1), ('wordsworth', 1)]

2-
Iteration = 1 training score (micro, macro) =  (0.82046864231564454, 0.73425683013887211)
Iteration = 2 training score (micro, macro) =  (0.80806340454858716, 0.74456037758605786)
Iteration = 3 training score (micro, macro) =  (0.86457615437629221, 0.81442692796003602)
Iteration = 4 training score (micro, macro) =  (0.7911784975878704, 0.71373094770884371)
Iteration = 5 training score (micro, macro) =  (0.89972432804962099, 0.87542249190820309)
Iteration = 6 training score (micro, macro) =  (0.92763611302549964, 0.9088732978148748)
Iteration = 7 training score (micro, macro) =  (0.85182632667126124, 0.81010887471944715)
Iteration = 8 training score (micro, macro) =  (0.97656788421778085, 0.96489939143013048)
Iteration = 9 training score (micro, macro) =  (0.94348725017229496, 0.9126982228176902)
Iteration = 10 training score (micro, macro) =  (0.9490006891798759, 0.9190183124564415)
Iteration = 11 training score (micro, macro) =  (0.97036526533425227, 0.9551653827583001)
Iteration = 12 training score (micro, macro) =  (0.98552722260509995, 0.97969254808273387)
Iteration = 13 training score (micro, macro) =  (0.98173673328738786, 0.97195835771850936)
Iteration = 14 training score (micro, macro) =  (0.96898690558235701, 0.95406559484322584)
Iteration = 15 training score (micro, macro) =  (0.96485182632667121, 0.95115685033271546)
Iteration = 16 training score (micro, macro) =  (0.98242591316333561, 0.97255695835887968)
Iteration = 17 training score (micro, macro) =  (0.99448656099241906, 0.99093032206893028)
Iteration = 18 training score (micro, macro) =  (0.99655410062026184, 0.99438097185357621)
Iteration = 19 training score (micro, macro) =  (0.99414197105444524, 0.99027974759341186)
Iteration = 20 training score (micro, macro) =  (0.99758787043418329, 0.9965354874902338)

3- classifier f-measures on the test set:
micro averaged = 0.8266
macro averaged = 0.7255

4-
random shuffling: since this is a online update, random shuffling would help generalize.
learning rate: from the lecture slides, we set the learning rate to 1. but we also tried
other learning rates with no significant improvement.
number of iterations: we used 20 iteration.
weight averaging: weight averaging should help generalized. But we don't see significant improvement
---------------------------------------------------------------------------------
Part 4)
A) Feature A:

1- My new feature is part of speech which use nltk.pos_tag() function to
find all POS in a window centered at the target.

2- naive-bayes f-measures on the test set:
micro averaged = 0.8523
macro averaged = 0.7737

3- perceptron f-measures on the test set:
micro averaged = 0.8170
macro averaged = 0.7102

4- Conclusions:
Above scores are obtained from concatinating my new feature with the traditional
bag of words feature. The f score does not have significant changes comparing to
using traditional feature along. I also used my new feature along. The f score
obtained is similar to the baseline frequency count (around 57%). My new feature
vocabulary size is only around 40 which may not be enough to separate data. Also
POS is better used with relative location information. I think the result from using
new feature alone is complete noise, and the 56% score is mainly due to the prior p(s).

B) Feature B:

1- Description

2- naive-bayes f-measures on the test set:
micro averaged =
macro averaged =

3- perceptron f-measures on the test set:
micro averaged =
macro averaged =


4- Conclusions:
